{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The majority of this code is copied directly from the tensorflow website. My real contribution is just in the data wrangling/prep that happened prior to this model-fitting & evaluating"
      ],
      "metadata": {
        "id": "59ys7ifTEZi0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9Cra7TTKfQG",
        "outputId": "6fdd9574-c2aa-4b5a-9575-e280d69fc112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "tar_path = 'drive/MyDrive/Capstone'\n",
        "tar_name = '/cap_reviews.tar.gz'\n",
        "\n",
        "with tarfile.open(tar_path + tar_name, 'r:gz') as f:\n",
        "  f.extractall('keras')"
      ],
      "metadata": {
        "id": "1FcEW1JGLQkZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content')\n",
        "\n",
        "main_directory = os.path.abspath('keras/keras')\n",
        "train_year = '2020'\n",
        "other_years = os.listdir(main_directory).remove(train_year)\n",
        "train_dir = os.path.join(main_directory, train_year, \"train\")\n",
        "\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds =  tf.keras.utils.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size = batch_size,\n",
        "    #validation_split = 0.2,\n",
        "    #subset = 'training',\n",
        "    seed = seed\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sui-DtnsNSV-",
        "outputId": "21adad42-71b9-4d00-b935-7f187cf37e41"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this will be altered according to overfit conditions\n",
        "'''\n",
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split = 0.2,\n",
        "    subset='validation',\n",
        "    seed=seed\n",
        ")'''"
      ],
      "metadata": {
        "id": "yCZhOh2XrRSh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2813554a-e4b6-4406-86c3-10469eef1481"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nraw_val_ds = tf.keras.utils.text_dataset_from_directory(\\n    train_dir,\\n    batch_size=batch_size,\\n    validation_split = 0.2,\\n    subset='validation',\\n    seed=seed\\n)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this I need to hold somewhat hostage and come back to\n",
        "raw_test_dss = {}\n",
        "other_years = [year for year in os.listdir(main_directory) if year != train_year and year not in ['2008','2009','2010']]\n",
        "\n",
        "for y in other_years:\n",
        "  test_dir = os.path.join(main_directory, y, \"test\")\n",
        "\n",
        "  raw_test_dss[y] = (tf.keras.utils.text_dataset_from_directory(\n",
        "      test_dir,\n",
        "      batch_size=batch_size\n",
        "  ))"
      ],
      "metadata": {
        "id": "DL7eNa8GrgA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b718ffa-b58b-408f-ddb4-c8cd78ad9863"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "YJXQCPGHrmQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')"
      ],
      "metadata": {
        "id": "xRG27RGfrpsq"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10_000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize = custom_standardization,\n",
        "    max_tokens = max_features,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = sequence_length\n",
        ")"
      ],
      "metadata": {
        "id": "Xs80yn7Drscs"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make a text-only dataset then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "metadata": {
        "id": "wSI42z1Srul7"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "metadata": {
        "id": "NbVfAP1EryiA"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we apply these preprocessing steps\n",
        "\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "#val_ds = raw_val_ds.map(vectorize_text)\n",
        "\n",
        "test_dss = {}\n",
        "for year in raw_test_dss.keys():\n",
        "  test_dss[year] = raw_test_dss[year].map(vectorize_text)"
      ],
      "metadata": {
        "id": "rr3EJ0thr3iX"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is a performance-enhancing step. caching allows the data to be stored on-disk in one large file and .prefetch() overlaps preprocessing and model execution\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "#val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "for year in test_dss.keys():\n",
        "  test_dss[year] = test_dss[year].cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "H1ZSB5ELr-Xy"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## time for the model"
      ],
      "metadata": {
        "id": "-uPcwj57sCyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Embedding(max_features + 1, embedding_dim),\n",
        "    #layers.Dropout(0.2),\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "    #layers.Dropout(0.2),\n",
        "    layers.Dense(1)]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "HyRpW2KLsFKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a2d4780-f204-4408-f7a8-757f9523fb9c"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_18 (Embedding)    (None, None, 16)          160016    \n",
            "                                                                 \n",
            " global_average_pooling1d_18  (None, 16)               0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
      ],
      "metadata": {
        "id": "k080BoxasI_a"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "K2giuIpysQwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "\n",
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    #validation_data=val_ds,\n",
        "    epochs=epochs\n",
        ")"
      ],
      "metadata": {
        "id": "-meSXYj2sLAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eecf36eb-9a3c-4bcb-cbc3-1044f9bdec55"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6092 - binary_accuracy: 0.6823\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.4474 - binary_accuracy: 0.8270\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.3346 - binary_accuracy: 0.8934\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2719 - binary_accuracy: 0.9155\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2338 - binary_accuracy: 0.9266\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2086 - binary_accuracy: 0.9339\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.1906 - binary_accuracy: 0.9378\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.1768 - binary_accuracy: 0.9412\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.1657 - binary_accuracy: 0.9445\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.1563 - binary_accuracy: 0.9476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "JeFVmdphsM_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#os.chdir('drive/MyDrive/Capstone')\n",
        "\n",
        "for year in test_dss.keys():\n",
        "  loss, accuracy = model.evaluate(test_dss[year])\n",
        "\n",
        "  print(year, \": Loss: \", loss)\n",
        "  print(year, \": Accuracy: \", accuracy)\n",
        "\n",
        "  history_dict = history.history\n",
        "\n",
        "  acc=history_dict['binary_accuracy']\n",
        "  #val_acc=history_dict['val_binary_accuracy']\n",
        "  loss=history_dict['loss']\n",
        "  #val_loss=history_dict['val_loss']\n",
        "\n",
        "  epochs = range(1, len(acc) + 1)\n"
      ],
      "metadata": {
        "id": "l_-VuwajsTTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "873b391f-b4e2-4cb3-da56-8c0152f62788"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2263 - binary_accuracy: 0.9165\n",
            "2018 : Loss:  0.22633349895477295\n",
            "2018 : Accuracy:  0.9164800047874451\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2770 - binary_accuracy: 0.8926\n",
            "2014 : Loss:  0.2769565284252167\n",
            "2014 : Accuracy:  0.8925999999046326\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2193 - binary_accuracy: 0.9195\n",
            "2019 : Loss:  0.2192772626876831\n",
            "2019 : Accuracy:  0.9195200204849243\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.3545 - binary_accuracy: 0.8618\n",
            "2012 : Loss:  0.3545071482658386\n",
            "2012 : Accuracy:  0.8618000149726868\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2530 - binary_accuracy: 0.9056\n",
            "2016 : Loss:  0.2530452013015747\n",
            "2016 : Accuracy:  0.9056400060653687\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2351 - binary_accuracy: 0.9102\n",
            "2017 : Loss:  0.23514722287654877\n",
            "2017 : Accuracy:  0.9101999998092651\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2707 - binary_accuracy: 0.8986\n",
            "2015 : Loss:  0.2706727981567383\n",
            "2015 : Accuracy:  0.8985999822616577\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.3101 - binary_accuracy: 0.8799\n",
            "2013 : Loss:  0.31008559465408325\n",
            "2013 : Accuracy:  0.8798800110816956\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.3898 - binary_accuracy: 0.8459\n",
            "2011 : Loss:  0.38981765508651733\n",
            "2011 : Accuracy:  0.8458799719810486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # \"bo\" is for blue dot\n",
        "  plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "  # \"b\" is for solid blue line\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.savefig(\"med_losses.png\")\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend(loc='lower right')\n",
        "\n",
        "  plt.savefig(\"med_acc.png\")"
      ],
      "metadata": {
        "id": "NK3l8nKmJTlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the history can show us what happened during training (it was recorded by model.fit)\n",
        "\n"
      ],
      "metadata": {
        "id": "0nEuQ1QAsVPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKuf4Tz8sXWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N0DDUflusZef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now you can export the model"
      ],
      "metadata": {
        "id": "K1m-wIHisdIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    model,\n",
        "    layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer='adam', metrics=['accuracy']\n",
        ")\n",
        "\n",
        "#Test it with 'raw_test_ds', which yields raw strings\n",
        "for year in raw_test_dss.keys():\n",
        "  loss, accuracy = export_model.evaluate(raw_test_dss[year])\n",
        "  print(accuracy)"
      ],
      "metadata": {
        "id": "VJElPcuasbp7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "476991e4-84ba-45b4-b5ca-c7967e722d49"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 13s 16ms/step - loss: 0.3473 - accuracy: 0.8552\n",
            "0.855239987373352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    model,\n",
        "    layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer='adam', metrics=['accuracy']\n",
        ")\n",
        "\n",
        "#Test it with 'raw_test_ds', which yields raw strings\n",
        "for year in raw_test_dss.keys():\n",
        "  loss, accuracy = export_model.evaluate(raw_test_dss[year])\n",
        "  print(accuracy)"
      ],
      "metadata": {
        "id": "djQvhJ1CshkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# this stuff is to tell me what variables were set during the course of this script. \n",
        "\n",
        "all_variables = dir()\n",
        "\n",
        "# Iterate over the whole list where dir( )\n",
        "# is stored.\n",
        "for name in all_variables:\n",
        "\n",
        "# Print the item if it doesn't start with '__'\n",
        "  if not name.startswith('__'):\n",
        "    myvalue = eval(name)\n",
        "     print(name, \"is\", type(myvalue), \"and is equal to \", myvalue)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IUzcLIq2skZx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}